{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e584649",
   "metadata": {},
   "source": [
    "Web -> Bedrock -> Translate -> Comprehend -> S3\n",
    "\n",
    "1. Use `requests` to scrap web pages and extract the text using `BeautifulSoup`.\n",
    "\n",
    "2. Send the text to Amazon Bedrock for cleaning/summarizing.\n",
    "\n",
    "3. Translate to English using Amazon Translate.\n",
    "\n",
    "4. Perform sentiment analysis using Amazon Comprehend.\n",
    "\n",
    "5. Serialize the results and upload them to S3 as json files.\n",
    "\n",
    "Before running, please run `pip install -r requirements.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e68732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import logging\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, BotoCoreError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"web-pipeline\")\n",
    "\n",
    "# Environment variable configuration (can be done using AWS Configure or Environment Variables)\n",
    "AWS_REGION = os.environ.get(\"AWS_REGION\", \"eu-west-1\")\n",
    "S3_BUCKET = os.environ.get(\"RESULT_S3_BUCKET\", \"ceu-jiaqi-2025\")\n",
    "BEDROCK_MODEL_ID = os.environ.get(\"BEDROCK_MODEL_ID\", \"\")  # 若不使用 Bedrock 可留空\n",
    "\n",
    "# boto3 clients\n",
    "s3 = boto3.client(\"s3\", region_name=AWS_REGION)\n",
    "translate = boto3.client(\"translate\", region_name=AWS_REGION)\n",
    "comprehend = boto3.client(\"comprehend\", region_name=AWS_REGION)\n",
    "# The Bedrock client name in boto3 is 'bedrock-runtime' (SDK support required).\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec72fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Use `requests` to scrap web pages and extract the text using `BeautifulSoup`.\n",
    "\n",
    "\n",
    "def fetch_page_text(url: str, timeout: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Use requests + BeautifulSoup for simple text extraction.\n",
    "\n",
    "    Strategy: First try the `<article>` tag; if none is found, merge all `<p>` paragraphs.\n",
    "\n",
    "    Return to plain text (removing extra whitespace).\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; web-pipeline/1.0)\"}\n",
    "    resp = requests.get(url, timeout=timeout, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Try common text containers.\n",
    "    candidates = []\n",
    "    article = soup.find(\"article\")\n",
    "    if article:\n",
    "        candidates.append(article.get_text(separator=\"\\n\", strip=True))\n",
    "\n",
    "    # Common class names (simplified)\n",
    "    for cls in (\"main\", \"content\", \"post\", \"article-body\", \"entry-content\"):\n",
    "        el = soup.find(class_=cls)\n",
    "        if el:\n",
    "            candidates.append(el.get_text(separator=\"\\n\", strip=True))\n",
    "\n",
    "    # fallback: merge all `<p>` paragraphs\n",
    "    if not candidates:\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \"\\n\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "    else:\n",
    "        # Select the longest candidate.\n",
    "        text = max(candidates, key=len)\n",
    "\n",
    "    # Clean up extra whitespace\n",
    "    text = \"\\n\".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Send the text to Amazon Bedrock for cleaning/summarizing.\n",
    "\n",
    "def bedrock_clean_text(text: str, model_id: str = BEDROCK_MODEL_ID, timeout_seconds: int = 60) -> str:\n",
    "    \"\"\"\n",
    "    If `BEDROCK_MODEL_ID` is configured, the `text` is sent to the Bedrock model, which then returns the cleaned text.\n",
    "    If `model_id` is not configured, the original text is returned directly.\n",
    "    \"\"\"\n",
    "    if not model_id:\n",
    "        logger.info(\"No Bedrock model configured, skipping Bedrock step.\")\n",
    "        return text\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a text-cleaning assistant.\\n\"\n",
    "        \"Given a noisy HTML-extracted article text, return a cleaned, readable plaintext article.\\n\\n\"\n",
    "        \"ARTICLE:\\n\" + text + \"\\n\\nCLEANED ARTICLE:\\n\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = bedrock.invoke_model(\n",
    "            modelId=model_id,\n",
    "            contentType=\"text/plain; charset=utf-8\",\n",
    "            accept=\"application/json\",\n",
    "            body=prompt.encode(\"utf-8\")\n",
    "        )\n",
    "        body_bytes = response[\"body\"].read()\n",
    "        cleaned = body_bytes.decode(\"utf-8\")\n",
    "        return cleaned\n",
    "    except (ClientError, BotoCoreError) as e:\n",
    "        logger.exception(\"Bedrock invocation failed, returning original text: %s\", e)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10280965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Segment long texts (because Translate/Comprehend has length limitations).\n",
    "\n",
    "def split_text_chunks(text: str, max_chars: int = 4500) -> List[str]:\n",
    "    \"\"\"\n",
    "    Segment by paragraph and combine into blocks not exceeding max_chars=4500.\n",
    "    \"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    chunks = []\n",
    "    cur = []\n",
    "    cur_len = 0\n",
    "    for p in paragraphs:\n",
    "        if len(p) > max_chars:\n",
    "            # If a single paragraph is too long, then it can be split into sentences or fixed lengths.\n",
    "            for i in range(0, len(p), max_chars):\n",
    "                piece = p[i:i+max_chars]\n",
    "                if cur:\n",
    "                    chunks.append('\\n\\n'.join(cur))\n",
    "                    cur = []\n",
    "                    cur_len = 0\n",
    "                chunks.append(piece)\n",
    "        else:\n",
    "            if cur_len + len(p) + 2 > max_chars:\n",
    "                chunks.append('\\n\\n'.join(cur))\n",
    "                cur = [p]\n",
    "                cur_len = len(p)\n",
    "            else:\n",
    "                cur.append(p)\n",
    "                cur_len += len(p) + 2\n",
    "    if cur:\n",
    "        chunks.append('\\n\\n'.join(cur))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Translation (send each chunk to Translate, then reassemble them)\n",
    "\n",
    "def translate_chunks_to_english(chunks: List[str]) -> Tuple[str, str]:\n",
    "    \"\"\"Translate each text block into English and merge them, returning (full_translated_text, detected_source_language). \n",
    "    SourceLanguageCode is adjusted manually.\n",
    "    \"\"\"\n",
    "    translated_parts = []\n",
    "    detected_lang = None\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            resp = translate.translate_text(Text=chunk, SourceLanguageCode=\"zh\",TargetLanguageCode=\"en\")\n",
    "            translated_parts.append(resp.get(\"TranslatedText\", \"\"))\n",
    "            if detected_lang is None:\n",
    "                detected_lang = resp.get(\"SourceLanguageCode\")\n",
    "        except (ClientError, BotoCoreError) as e:\n",
    "            logger.exception(\"Translate failed for a chunk, adding original chunk instead: %s\", e)\n",
    "            translated_parts.append(chunk)\n",
    "    return \"\\n\\n\".join(translated_parts), (detected_lang or \"unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Sentiment analysis (using Comprehend on translated English text)\n",
    "\n",
    "def analyze_sentiment_for_text(text: str, language_code: str = \"en\") -> Dict[str, Any]:\n",
    "    \"\"\"If the text is too long, call `detect_sentiment` block by block, then merge the scores.\n",
    "    Returns the overall sentiment label (majority or score-based synthesis) and details for each block.\n",
    "    \"\"\"\n",
    "    chunks = split_text_chunks(text, max_chars=4500)\n",
    "    results = []\n",
    "    # Comprehend detect_sentiment processes a piece of text each time.\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            resp = comprehend.detect_sentiment(Text=chunk, LanguageCode=language_code)\n",
    "            results.append(resp)\n",
    "        except (ClientError, BotoCoreError) as e:\n",
    "            logger.exception(\"Comprehend detect_sentiment failed for a chunk: %s\", e)\n",
    "            results.append({\"Sentiment\": \"ERROR\", \"SentimentScore\": {}})\n",
    "\n",
    "    # Merging logic: Averaging of all sentiment scores\n",
    "    score_sum = {\"Positive\": 0.0, \"Negative\": 0.0, \"Neutral\": 0.0, \"Mixed\": 0.0}\n",
    "    valid = 0\n",
    "    for r in results:\n",
    "        sc = r.get(\"SentimentScore\") or {}\n",
    "        if sc:\n",
    "            valid += 1\n",
    "            for k in score_sum.keys():\n",
    "                score_sum[k] += float(sc.get(k, 0.0))\n",
    "    if valid > 0:\n",
    "        avg_score = {k: v / valid for k, v in score_sum.items()}\n",
    "        # The sentiment with the highest average score was selected as the overall sentiment.\n",
    "        overall_sentiment = max(avg_score.items(), key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        avg_score = {}\n",
    "        overall_sentiment = \"UNKNOWN\"\n",
    "\n",
    "    return {\n",
    "        \"overall_sentiment\": overall_sentiment,\n",
    "        \"average_scores\": avg_score,\n",
    "        \"per_chunk\": results,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Upload JSON to S3\n",
    "\n",
    "def upload_json_to_s3(obj: Dict[str, Any], key: str) -> str:\n",
    "    body = json.dumps(obj, ensure_ascii=False, indent=None).encode(\"utf-8\")\n",
    "    try:\n",
    "        s3.put_object(Bucket=S3_BUCKET, Key=key, Body=body, ContentType=\"application/json; charset=utf-8\")\n",
    "    except ClientError:\n",
    "        logger.exception(\"Failed to upload result to S3\")\n",
    "        raise\n",
    "    return f\"s3://{S3_BUCKET}/{key}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. String all the steps together into a single `process_url` function.\n",
    "\n",
    "def process_url(url: str, do_bedrock_clean: bool = True) -> Dict[str, Any]:\n",
    "    start = time.time()\n",
    "    logger.info(\"Processing URL: %s\", url)\n",
    "    raw_text = fetch_page_text(url)\n",
    "    if not raw_text or len(raw_text.strip()) == 0:\n",
    "        logger.warning(\"No textual content extracted from %s\", url)\n",
    "        return {\"url\": url, \"status\": \"no_content\"}\n",
    "\n",
    "    cleaned_text = raw_text\n",
    "    if do_bedrock_clean and BEDROCK_MODEL_ID:\n",
    "        cleaned_text = bedrock_clean_text(raw_text, model_id=BEDROCK_MODEL_ID)\n",
    "\n",
    "    chunks = split_text_chunks(cleaned_text)\n",
    "    translated_text, src_lang = translate_chunks_to_english(chunks)\n",
    "\n",
    "\n",
    "    sentiment = analyze_sentiment_for_text(translated_text, language_code=\"en\")\n",
    "\n",
    "    result = {\n",
    "        \"source_url\": url,\n",
    "        \"detected_source_language\": src_lang,\n",
    "        \"raw_text_excerpt\": raw_text[:2000],\n",
    "        \"cleaned_text_excerpt\": cleaned_text[:2000],\n",
    "        \"translated_text_excerpt\": translated_text[:2000],\n",
    "        \"sentiment\": sentiment,\n",
    "        \"meta\": {\n",
    "            \"char_counts\": {\n",
    "                \"raw\": len(raw_text),\n",
    "                \"cleaned\": len(cleaned_text),\n",
    "                \"translated\": len(translated_text),\n",
    "            },\n",
    "            \"processing_time_sec\": time.time() - start,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Use the URL's SHA1 hash + timestamp as the object key.\n",
    "    key = f\"results/{hashlib.sha1(url.encode()).hexdigest()}_{int(time.time())}.json\"\n",
    "    s3_uri = upload_json_to_s3(result, key)\n",
    "    result[\"s3_uri\"] = s3_uri\n",
    "    logger.info(\"Finished %s -> %s\", url, s3_uri)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Demo: Handling single or multiple URLs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Put a list of URLs to be processed into urls.\n",
    "    urls = [\n",
    "        \"https://example.com/some-article\",\n",
    "    ]\n",
    "    for u in urls:\n",
    "        try:\n",
    "            out = process_url(u, do_bedrock_clean=False) \n",
    "            print(json.dumps(out, ensure_ascii=False, indent=2))\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error processing %s: %s\", u, e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
